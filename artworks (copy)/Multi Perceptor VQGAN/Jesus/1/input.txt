https://colab.research.google.com/gist/joetm/b4d166ce88f725e09291080fee5bec13/multi-perceptor-vqgan-clip-public.ipynb#scrollTo=YWGR8q9AT8uk


What do you want to see?

text_prompt:
Jesus is here. Praise the lord. Massive resurrection. by John Glover. Trending on artstation
gen_seed:
-1
If you want to keep starting from the same point, set gen_seed to a positive number. -1 will make it random every time.
init_image:
Insert text here
width:
500
height:
412
max_iter:
600
There are different ways of generating the random starting point, when not using an init image. These influence how the image turns out. The default VQGAN ZRand is good, but some models and subjects may do better with perlin or pyramid noise.

rand_init_mode:

Perlin Noise
perlin_octaves:

7
perlin_weight:

0.22
pyramid_octaves:

5
pyramid_decay:

0.99
How many slices of the image should be sent to CLIP each iteration to score? Higher numbers are better, but cost more memory. If you are running into memory issues try lowering this value.

cut_n:
64
One clip model is good. Two is better? You may need to reduce the number of cuts to support having more than one CLIP model. CLIP is what scores the image against your prompt and each model has slightly different ideas of what things are.

ViT-B/32 is fast and good and what most people use to begin with
clip_model:

ViT-B/32
clip_model2:

None
clip1_weight:

0.5
Picking a different VQGAN model will impact how an image generates. Think of this as giving the generator a different set of brushes and paints to work with. CLIP is still the "eyes" and is judging the image against your prompt but using different brushes will make a different image.

vqgan_imagenet_f16_16384 is the default and what most people use
vqgan_model:

vqgan_imagenet_f16_16384
Learning rates greatly impact how quickly an image can generate, or if an image can generate at all. The first learning rate is only for the first 50 iterations. The epoch rate is what is used after reaching the first mse epoch. You can try lowering the epoch rate while raising the initial learning rate and see what happens

learning_rate:
0.2
learning_rate_epoch:
0.2
How much should we try to match the init image, or if no init image how much should we resist change after reaching the first epoch?

mse_weight:
0.5
Adding some TV may make the image blurrier but also helps to get rid of noise. A good value to try might be 0.1.

tv_weight:
0.0
Should the total weight of the text prompts stay in the same range, relative to other loss functions?

normalize_prompt_weights:

Enabling the EMA tensor will cause the image to be slower to generate but may help it be more cohesive. This can also help keep the final image closer to the init image, if you are providing one.

use_ema_tensor:

If you want to generate a video of the run, you need to save the frames as you go. The more frequently you save, the longer the video but the slower it will take to generate.

save_art_output:

save_frames_for_video:

save_frequency_for_video:
3
