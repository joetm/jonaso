{"docs":[{"title":"A General Language Assistant as a Laboratory for Alignment","priority":3},{"title":"Training language models to follow instructions with human feedback","priority":3},{"title":"Rebel AI group raises record cash after machine learning schism","priority":0},{"title":"Predictability and Surprise in Large Generative Models","priority":2},{"title":"The Capacity for Moral Self-Correction in Large Language Models","priority":3},{"title":"B E Y O N D T H E I M I TAT I O N G A M E : Q U A N T I F Y- I N G A N D E X T R A P O L AT I N G T H E C A PA B I L I T I E S O F L A N G U A G E M O D E L S","priority":1},{"title":"TOWARDS UNDERSTANDING SYCOPHANCY IN LANGUAGE MODELS","priority":2},{"title":"Discovering Language Model Behaviors with Model-Written Evaluations","priority":2},{"title":"In-context Learning and Induction Heads","priority":2},{"title":"Constitutional AI: Harmlessness from AI Feedback","priority":3},{"title":"Learning Transferable Visual Models From Natural Language Supervision","priority":2}],"keywords":["AI","Issues","Human-AI Alignment","Alignment with Human Values","Helpful, Honest, Harmless (HHH)","InstructGPT","Hype","Anthropic","Predictability and Surprise","Prompt Engineering","Prompt Techniques","Moral Self-Correction","Benchmarks, gold standards, datasets","NLP","Beyond the Imitation Game (BIG-Bench)","Sycophancy, Sandbagging","Scaling","Inverse Scaling","Model-Written Evaluations","In-Context Learning","Induction Heads","Machine Learning","Reinforcement Learning","Reinforcement Learning with Human Feedback (RLHF)","Constitutional AI","Contrastive Learning, Multimodal Models","CLIP"]}