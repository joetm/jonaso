{"docs":[{"title":"Predictability and Surprise in Large Generative Models","priority":2},{"title":"Learning Transferable Visual Models From Natural Language Supervision","priority":2},{"title":"Rebel AI group raises record cash after machine learning schism","priority":0},{"title":"Training language models to follow instructions with human feedback","priority":3},{"title":"The Capacity for Moral Self-Correction in Large Language Models","priority":3},{"title":"Discovering Language Model Behaviors with Model-Written Evaluations","priority":3},{"title":"A General Language Assistant as a Laboratory for Alignment","priority":3},{"title":"Constitutional AI: Harmlessness from AI Feedback","priority":3}],"keywords":["AI","Issues","Predictability and Surprise","Machine Learning","Classification","OpenAI CLIP","Hype","Anthropic","Human-AI Alignment","InstructGPT","Prompt Engineering","Prompt Techniques","Moral Self-Correction","NLP","Evaluation","Model-Written Evaluations","Inverse Scaling in RLHF","Alignment with Human Values","Helpful, Honest, Harmless (HHH)","Reinforcement Learning","Reinforcement Learning with Human Feedback (RLHF)","Constitutional AI"]}