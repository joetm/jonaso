{"docs":[{"title":"WHAT\u2019S IN MY BIG DATA?","priority":3},{"title":"Investigating the \"Wisdom of Crowds\" at Scale","priority":0},{"title":"Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","priority":2},{"title":"OLMo : Accelerating the Science of Language Models","priority":1},{"title":"THE UNLOCKING SPELL ON BASE LLMS: RETHINKING ALIGNMENT VIA IN-CONTEXT LEARNING","priority":3}],"keywords":["NLP","Training","Training Data","What's in My Big Data (WIMBD)","Collective Intelligence","Benchmarks, gold standards, datasets","Dolma","Language Models","OLMo (AI2)","AI","Issues","Human-AI Alignment","Alignment Tuning","URIAL (Untuned LLMs with Restyled In-context ALignment)"]}