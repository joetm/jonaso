{"docs":[{"title":"LARGE LANGUAGE MODELS","priority":3},{"title":"LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS","priority":1},{"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models","priority":3},{"title":"LARGE LANGUAGE MODELS CAN SELF-IMPROVE","priority":2},{"title":"SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS","priority":2},{"title":"UL2: Unifying Language Learning Paradigms","priority":2},{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1},{"title":"Scaling Instruction-Finetuned Language Models","priority":1},{"title":"RECITATION-AUGMENTED LANGUAGE MODELS","priority":3},{"title":"Chain-of-Thought Reasoning Without Prompting","priority":3}],"keywords":["Prompt Engineering","Applications","Optimization","Optimization by Prompting (OPRO)","Prompt Techniques","Least-to-most Prompting","Chain of Thought, CoT","Self-Improvement","Self-Consistency, CoT-SC","NLP","Training","Unified Language Learning","UL2 (Google)","Language Models","PaLM (Google)","Flan-T5","Instruction Finetuning","Augmented Language Models","Recitation-Augmented LLMs","Prompt Engineering Techniques","CoT-Decoding"]}