{"docs":[{"title":"LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS","priority":1},{"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models","priority":2},{"title":"LARGE LANGUAGE MODELS CAN SELF-IMPROVE","priority":2},{"title":"SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS","priority":1},{"title":"UL2: Unifying Language Learning Paradigms","priority":2},{"title":"Scaling Instruction-Finetuned Language Models","priority":1},{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1}],"keywords":["Prompt Engineering","Prompt Techniques","Least-to-most Prompting","Chain of Thought Prompting","Self-Improvement","Self-Consistency","NLP","Training","Unified Language Learning","UL2 (Google)","Language Models","Flan-T5","Instruction Finetuning","Google PaLM"]}