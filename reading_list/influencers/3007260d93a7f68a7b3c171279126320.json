{"docs":[{"title":"Evaluating Large Language Models Trained on Code","priority":3},{"title":"Language Models are Few-Shot Learners","priority":3},{"title":"Scaling Laws for Autoregressive Generative Modeling","priority":0},{"title":"Learning Transferable Visual Models From Natural Language Supervision","priority":2},{"title":"Robust Speech Recognition via Large-Scale Weak Supervision","priority":1},{"title":"Zero-Shot Text-to-Image Generation","priority":2},{"title":"Generative Pretraining from Pixels","priority":0}],"keywords":["NLP","Language Models","Codex (OpenAI)","GPT","GPT-3","Issues","Scaling Laws","Machine Learning","Classification","OpenAI CLIP","Applications","Speech Recognition","Speech Recognition Models","Whisper","Generative Deep Learning","Text-to-Image Models","Dall-E","Systems, Libraries, Services, Tools","Deep Learning","Image GPT"]}