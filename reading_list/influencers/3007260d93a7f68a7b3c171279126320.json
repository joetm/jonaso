{"docs":[{"title":"Scaling Laws for Autoregressive Generative Modeling","priority":0},{"title":"Evaluating Large Language Models Trained on Code","priority":3},{"title":"Language Models are Few-Shot Learners","priority":3},{"title":"Robust Speech Recognition via Large-Scale Weak Supervision","priority":1},{"title":"Zero-Shot Text-to-Image Generation","priority":2},{"title":"Jukebox: A Generative Model for Music","priority":1},{"title":"Learning to summarize from human feedback","priority":2},{"title":"Generative Pretraining from Pixels","priority":0},{"title":"Learning Transferable Visual Models From Natural Language Supervision","priority":2},{"title":"Text and Code Embeddings by Contrastive Pre-Training","priority":1},{"title":"Language Models are Unsupervised Multitask Learners","priority":2},{"title":"Improving Language Understanding by Generative Pre-Training","priority":2}],"keywords":["NLP","Issues","Scaling Laws","Language Models","Codex (OpenAI)","GPT","GPT-3","Machine Learning","Applications","Speech Recognition","Speech Recognition Models","Whisper","Generative Deep Learning","Text-to-Image","Dall-E","Audio","Text-to-Music","Jukebox","Reinforcement Learning","Reinforcement Learning with Human Feedback (RLHF)","Systems, Libraries, Services, Tools","Deep Learning","Image GPT","Contrastive Learning, Multimodal Models","CLIP","Embeddings","Few-shot","GPT-2"]}