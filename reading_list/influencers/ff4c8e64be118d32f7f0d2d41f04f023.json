{"docs":[{"title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","priority":1},{"title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling","priority":2},{"title":"Pretraining Language Models with Human Preferences","priority":2}],"keywords":["NLP","Language Models","GPT","GPT-Neo","GPT-NeoX-20B","Benchmarks, gold standards, datasets","The Pile","AI","Issues","Human-AI Alignment","Human Preferences"]}