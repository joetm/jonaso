{"docs":[{"title":"Pretraining Language Models with Human Preferences","priority":2},{"title":"B E Y O N D T H E I M I TAT I O N G A M E : Q U A N T I F Y- I N G A N D E X T R A P O L AT I N G T H E C A PA B I L I T I E S O F L A N G U A G E M O D E L S","priority":1},{"title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling","priority":2},{"title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","priority":1}],"keywords":["AI","Issues","Human-AI Alignment","Human Preferences","Benchmarks, gold standards, datasets","NLP","Beyond the Imitation Game (BIG-Bench)","The Pile","Language Models","GPT","GPT-Neo","GPT-NeoX-20B"]}