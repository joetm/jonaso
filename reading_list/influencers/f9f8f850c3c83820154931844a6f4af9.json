{"docs":[{"title":"LLMs are (mostly) not helped by filler tokens","priority":1},{"title":"SLEEPER AGENTS: TRAINING DECEPTIVE LLMS","priority":1},{"title":"ALIGNMENT FAKING IN LARGE LANGUAGE MODELS","priority":2},{"title":"Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety","priority":3}],"keywords":["NLP","General, Theory","Filler Tokens","Agents, Autonomous Task Management","Issues","Sleeper Agents","AI","Human-AI Alignment","Alignment Faking","Prompt Engineering","Prompt Engineering Techniques","Chain of Thought, CoT","Chain of Thought Monitorability"]}