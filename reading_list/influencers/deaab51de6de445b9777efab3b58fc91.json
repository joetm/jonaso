{"docs":[{"title":"A General Language Assistant as a Laboratory for Alignment","priority":3},{"title":"Rebel AI group raises record cash after machine learning schism","priority":0},{"title":"Predictability and Surprise in Large Generative Models","priority":2},{"title":"The Capacity for Moral Self-Correction in Large Language Models","priority":3},{"title":"Measuring Faithfulness in Chain-of-Thought Reasoning","priority":2},{"title":"Scaling Laws for Autoregressive Generative Modeling","priority":0},{"title":"Discovering Language Model Behaviors with Model-Written Evaluations","priority":2},{"title":"In-context Learning and Induction Heads","priority":2},{"title":"Scaling Laws for Neural Language Models","priority":2},{"title":"Evaluating Large Language Models Trained on Code","priority":3},{"title":"Language Models are Few-Shot Learners","priority":3},{"title":"SLEEPER AGENTS: TRAINING DECEPTIVE LLMS","priority":1},{"title":"Scaling Laws for Transfer","priority":2},{"title":"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned","priority":3},{"title":"ALIGNMENT FAKING IN LARGE LANGUAGE MODELS","priority":2},{"title":"Sabotage Evaluations for Frontier Models","priority":1}],"keywords":["AI","Issues","Human-AI Alignment","Alignment with Human Values","Helpful, Honest, Harmless (HHH)","Hype","Anthropic","Predictability and Surprise","Prompt Engineering","Prompt Techniques","Moral Self-Correction","Chain of Thought, CoT","NLP","Scaling","Scaling Laws","Inverse Scaling","Model-Written Evaluations","In-Context Learning","Induction Heads","Language Models","Chinchilla (DeepMind)","Codex (OpenAI)","GPT","GPT-3","Few-Shot Learning","Agents, Autonomous Task Management","Sleeper Agents","Scaling Laws for Transfer","Red Teaming","Alignment Faking","Sabotage"]}