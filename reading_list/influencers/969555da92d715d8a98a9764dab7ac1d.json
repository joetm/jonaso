{"docs":[{"title":"QLORA: Efficient Finetuning of Quantized LLMs","priority":2},{"title":"Byte Latent Transformer: Patches Scale Better Than Tokens","priority":3}],"keywords":["Machine Learning","Generative Deep Learning","Text-to-Image","Stable Diffusion","Fine-tuning Stable Diffusion","Low-Rank Adaptation, LoRA","qLoRA","NLP","Transformers","Byte Latent Transformer"]}