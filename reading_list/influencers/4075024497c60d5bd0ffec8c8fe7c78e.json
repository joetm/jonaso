{"docs":[{"title":"Pretraining Language Models with Human Preferences","priority":2},{"title":"The Capacity for Moral Self-Correction in Large Language Models","priority":3},{"title":"Discovering Language Model Behaviors with Model-Written Evaluations","priority":2},{"title":"ELI5: Long Form Question Answering","priority":2},{"title":"Constitutional AI: Harmlessness from AI Feedback","priority":3},{"title":"Measuring Faithfulness in Chain-of-Thought Reasoning","priority":2},{"title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks","priority":1}],"keywords":["AI","Issues","Human-AI Alignment","Human Preferences","Prompt Engineering","Prompt Techniques","Moral Self-Correction","NLP","Scaling","Inverse Scaling","Model-Written Evaluations","Language Models","ELI5 BART","Machine Learning","Reinforcement Learning","Reinforcement Learning with Human Feedback (RLHF)","Constitutional AI","Chain of Thought, CoT","Augmented Language Models","Retireval Augmentation","Retrieval-Augmented Generation (RAG)"]}