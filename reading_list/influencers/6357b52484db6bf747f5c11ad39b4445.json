{"docs":[{"title":"It\u2019s Getting Crowded! How to Use Crowdsourcing Effectively for Web Science Research","priority":0},{"title":"On the state of reporting in crowdsourcing experiments and a checklist to aid current practices","priority":0},{"title":"Mechanical cheat: Spamming schemes and adversarial techniques on crowdsourcing platforms","priority":2},{"title":"On Task Abandonment in Crowdsourcing","priority":2},{"title":"Mechanical Cheat: Spamming Schemes and Adversarial Techniques on Crowdsourcing Platforms","priority":3},{"title":"How Does the Crowd Impact the Model?","priority":0},{"title":"Understanding Malicious Behavior in Crowdsourcing Platforms: The Case of Online Surveys","priority":3},{"title":"All That Glitters Is Gold \u2014 An Attack Scheme on Gold Questions in Crowdsourcing","priority":3},{"title":"An Integrated Socio-Technical Crowdsourcing Platform for Accelerating Returns in eScience","priority":3},{"title":"CrowdQ: Crowdsourced Query Understanding","priority":2},{"title":"Modus Operandi of CrowdWorkers: The Invisible Role of Microtask Work Environments","priority":1},{"title":"CrowdCO-OP: Sharing Risks and Rewards in Crowdsourcing","priority":3},{"title":"Scheduling Human Intelligence Tasks in Multi-Tenant Crowd-Powered Systems","priority":1},{"title":"The Dynamics of Micro-Task Crowdsourcing The Case of Amazon MTurk","priority":1},{"title":"\u201cWe Regret to Inform You\u201d\u2013 Managing Reactions to Rejection in Crowdsourcing","priority":1},{"title":"Understanding Worker Moods and Reactions to Rejection in Crowdsourcing","priority":1},{"title":"An Introduction to Hybrid Human-Machine Information Systems","priority":3},{"title":"From People to Entities: Typed Search in the Enterprise and the Web","priority":0},{"title":"Correct Me If I\u2019m Wrong: Fixing Grammatical Errors by Preposition Ranking","priority":0},{"title":"Iterative Human-in-the-Loop Discovery of Unknown Unknowns in Image Datasets","priority":0},{"title":"Generative AI in Crowdwork for Web and Social Media Research: A Survey of Workers at Three Platforms","priority":3}],"keywords":["Web Science","Crowdsourcing, Human Computation","Best Practices","Reporting Crowdsourcing Experiments","Checklist for Reporting Crowdsourcing Experiments","Issues","Bots, Automation","Abandonment","Adverserial Crowdsourcing","Bias","Recant","Malicious Use, Negative Side Effects","Tipping Point","Gold Questions","Applications","Citizen Science, eScience","Search, Querying, Human Flesh Engine, Search Support","Query Understanding","Task Design","Work Environment","Revenue Sharing, Subcontracting","Process, Workflow, Worker Selection","Planning, Scheduling","Dynamics","Motivation, Incentives","Rejection","Machine Learning","Active Learning, Interactive Machine Learning, Human-In-the-Loop","Hybrid-AI, H-AI","Information Retrieval","Generative AI"]}