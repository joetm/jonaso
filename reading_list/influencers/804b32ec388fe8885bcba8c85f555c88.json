{"docs":[{"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models","priority":1},{"title":"Crowdsourcing User Studies With Mechanical Turk","priority":3},{"title":"And Now for Something Completely Different: Improving Crowdsourcing Workflows with Micro-Diversions","priority":2},{"title":"Emergent Abilities of Large Language Models","priority":2},{"title":"He Says, She Says: Conflict and Coordination in Wikipedia","priority":1},{"title":"Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements","priority":0},{"title":"Social Information Foraging and Collaborative Search","priority":0},{"title":"Scaling Instruction-Finetuned Language Models","priority":1}],"keywords":["Prompt Engineering","Prompt Techniques","Chain of Thought Prompting","Crowdsourcing, Human Computation","Applications","User Studies","Process, Workflow, Worker Selection","Diversions","NLP","General, Theory","Emergent Abilities","Collective Intelligence","Machine Learning","Issues","Fairness","Fairness Metrics","Conditional Equality","Information Retrieval","Information Foraging","Social Information Foraging, Collaborative Search","Language Models","Flan-T5","Instruction Finetuning"]}