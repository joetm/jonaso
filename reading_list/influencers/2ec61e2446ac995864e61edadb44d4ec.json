{"docs":[{"title":"Break the Sequential Dependency of LLM Inference Using Lookahead Decoding","priority":2},{"title":"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena","priority":2},{"title":"RAFT: Adapting Language Model to Domain Speci c RAG","priority":3},{"title":"Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems","priority":3},{"title":"Efficient Memory Management for Large Language Model Serving with PagedAttention","priority":2}],"keywords":["NLP","Inference","Lookahead Decoding","Evaluation","Chatbot Arena","Augmented Language Models","Retrieval-Augmented Generation (RAG)","RAG Optimization, RAG Fine-Tuning, RAG Techniques","Retrieval-Augmented Fine-Tuning (RAFT)","Issues","Scaling","Inference Scaling","Scaling Law of Voting Inference Systems","vllm"]}