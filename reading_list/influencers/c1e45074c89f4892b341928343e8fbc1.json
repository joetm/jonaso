{"docs":[{"title":"Investigating and Mitigating Biases in Crowdsourced Data","priority":0},{"title":"How Crowd Worker Factors Influence Subjective Annotations: A Study of Tagging Misogynistic Hate Speech in Tweets","priority":1},{"title":"Towards Effective Crowd-Powered Online Content Moderation","priority":0},{"title":"How Context Influences Cross-Device Task Acceptance in Crowd Work","priority":0},{"title":"\u201cHi! I am the Crowd Tasker\u201d Crowdsourcing through Digital Voice Assistants","priority":2},{"title":"Combining Worker Factors for Heterogeneous Crowd Task Assignment","priority":0},{"title":"Effect of Cognitive Abilities on Crowdsourcing Task Performance","priority":1},{"title":"REGROW: Reimagining Global Crowdsourcing for Beter Human-AI Collaboration","priority":3}],"keywords":["Crowdsourcing, Human Computation","Issues","Bias","Annotating Misogynistic Hate Speech","Applications","Moderation","Mobile Crowdsourcing","Context","Speech-based","Process, Workflow, Worker Selection","Task Assignment","Task Recommendation, Routing","Andy","CHI22 Workshop"]}