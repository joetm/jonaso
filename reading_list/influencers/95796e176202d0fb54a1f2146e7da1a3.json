{"docs":[{"title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge","priority":1},{"title":"Bytes Are All You Need: Transformers Operating Directly On File Bytes","priority":1},{"title":"SAM-CLIP: MERGING VISION FOUNDATION MODELS TOWARDS SEMANTIC AND SPATIAL UNDERSTANDING","priority":1},{"title":"Weight Subcloning: Direct Initialization of Transformers Using Larger Pretrained Ones","priority":1},{"title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory","priority":1},{"title":"Speculative Streaming: Fast LLM Inference without Auxiliary Models","priority":2}],"keywords":["Benchmarks, gold standards, datasets","Search, Question-Answering","Visual Question-Answering","OK-VQA","NLP","Transformers","ByteFormer","Machine Learning","Contrastive Learning, Multimodal Models","CLIP","SAM-CLIP","Weight Subcloning","Inference","Memory","Constrained Memory","Speculative Streaming"]}