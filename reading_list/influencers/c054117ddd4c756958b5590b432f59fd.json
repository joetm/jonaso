{"docs":[{"title":"A General Language Assistant as a Laboratory for Alignment","priority":3},{"title":"Rebel AI group raises record cash after machine learning schism","priority":0},{"title":"Predictability and Surprise in Large Generative Models","priority":2},{"title":"The Malicious Use  of Artificial Intelligence: Forecasting, Prevention,  and Mitigation","priority":1},{"title":"Global AI Vibrancy Tool","priority":3},{"title":"The Capacity for Moral Self-Correction in Large Language Models","priority":3},{"title":"Discovering Language Model Behaviors with Model-Written Evaluations","priority":2},{"title":"Language Models are Few-Shot Learners","priority":3},{"title":"T HE S TAT E O F D E E PF A K E S","priority":2},{"title":"Learning Transferable Visual Models From Natural Language Supervision","priority":2},{"title":"In-context Learning and Induction Heads","priority":2}],"keywords":["AI","Issues","Human-AI Alignment","Alignment with Human Values","Helpful, Honest, Harmless (HHH)","Hype","Anthropic","Predictability and Surprise","Malicious Use","State of AI","AI index","2019","Prompt Engineering","Prompt Techniques","Moral Self-Correction","NLP","Scaling","Inverse Scaling","Model-Written Evaluations","Language Models","GPT","GPT-3","Machine Learning","Deepfakes","Contrastive Learning, Multimodal Models","CLIP","In-Context Learning","Induction Heads"]}