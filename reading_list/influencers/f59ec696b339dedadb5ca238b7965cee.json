{"docs":[{"title":"Predictability and Surprise in Large Generative Models","priority":2},{"title":"The Malicious Use  of Artificial Intelligence: Forecasting, Prevention,  and Mitigation","priority":1},{"title":"Evaluating Large Language Models Trained on Code","priority":3},{"title":"Language Models are Few-Shot Learners","priority":3},{"title":"Scaling Laws for Autoregressive Generative Modeling","priority":0},{"title":"The Capacity for Moral Self-Correction in Large Language Models","priority":3},{"title":"Discovering Language Model Behaviors with Model-Written Evaluations","priority":3},{"title":"A General Language Assistant as a Laboratory for Alignment","priority":3},{"title":"Learning to summarize from human feedback","priority":2}],"keywords":["AI","Issues","Predictability and Surprise","Malicious Use","NLP","Language Models","Codex (OpenAI)","GPT","GPT-3","Scaling Laws","Prompt Engineering","Prompt Techniques","Moral Self-Correction","Evaluation","Model-Written Evaluations","Inverse Scaling in RLHF","Human-AI Alignment","Alignment with Human Values","Helpful, Honest, Harmless (HHH)","Machine Learning","Reinforcement Learning","Reinforcement Learning with Human Feedback (RLHF)"]}