{"docs":[{"title":"Language Is Not All You Need: Aligning Perception with Language Models","priority":3},{"title":"Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers","priority":2},{"title":"Retentive Network: A Successor to Transformer for Large Language Models","priority":3},{"title":"TORCHSCALE: Transformers at Scale","priority":1},{"title":"KOSMOS-G: Generating Images in Context with Multimodal Large Language Models","priority":3},{"title":"KOSMOS-2.5: A Multimodal Literate Model","priority":3},{"title":"Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models","priority":2},{"title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","priority":2},{"title":"Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models","priority":3}],"keywords":["AI","Issues","Human-AI Alignment","Perception","Kosmos-1","NLP","In-Context Learning","Meta-Optimizer","Retentive Networks","Language Models","Foundation Architectures","TorchScale","Machine Learning","Contrastive Learning, Multimodal Models","Kosmos","Kosmos-G","Kosmos-2.5","Fine-Tuning","Instruction Tuning","Generalized Instruction Tuning (GLAN)","Optimization","Quantization","1-bit LLMs","BitNet","BitNet b1.58","Prompt Engineering","Prompt Engineering Techniques","Visualization-of-Thought"]}