{"docs":[{"title":"S TA R C O D E R : M AY T H E S O U R C E B E W I T H Y O U !","priority":2},{"title":"MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads","priority":1},{"title":"FLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-Awareness","priority":1},{"title":"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning","priority":1}],"keywords":["NLP","Language Models","Code Completion","StarCoder","Transformers","Speculative Decoding","Medusa","Attention","Flash Attention","FlashAttention-2"]}