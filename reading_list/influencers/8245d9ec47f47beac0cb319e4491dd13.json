{"docs":[{"title":"A LLM Assisted Exploitation of AI-Guardian","priority":2},{"title":"Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods","priority":1},{"title":"Extracting Training Data from Large Language Models","priority":1},{"title":"Extracting Training Data from Diffusion Models","priority":3},{"title":"Scalable Extraction of Training Data from (Production) Language Models","priority":3},{"title":"Universal and Transferable Adversarial Attacks on Aligned Language Models","priority":2},{"title":"Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models","priority":2},{"title":"Stealing Part of a Production Language Model","priority":1}],"keywords":["NLP","Issues","Adversarial Attacks","Adversarial Example Defense","AI-Guardian","Machine Learning","Adversarial Machine Learning","Private Training Data","Training Data Extraction Attack","Generative Deep Learning","Diffusion","Memorization of Training Data","Security Vulnerabilities","Training Data Extraction","Extractable Memorization","Jailbreaks","Greedy Coordinate Gradient (GCG)","Training Data Poisoning","Privacy Backdoors","Security","Model-Stealing Attacks","Last Layer"]}