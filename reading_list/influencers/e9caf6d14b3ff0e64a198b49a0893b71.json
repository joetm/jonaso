{"docs":[{"title":"LIMA: Less Is More for Alignment","priority":2},{"title":"DEMYSTIFYING CLIP DATA","priority":3},{"title":"Byte Latent Transformer: Patches Scale Better Than Tokens","priority":3}],"keywords":["AI","Issues","Human-AI Alignment","LIMA","Machine Learning","Contrastive Learning, Multimodal Models","CLIP","MetaCLIP,  Metadata-Curated Language-Image Pre-training","NLP","Transformers","Byte Latent Transformer"]}