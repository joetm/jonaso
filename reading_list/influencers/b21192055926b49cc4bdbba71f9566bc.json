{"docs":[{"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","priority":1},{"title":"OPT: Open Pre-trained Transformer Language Models","priority":3},{"title":"LLaMA: Open and Efficient Foundation Language Models","priority":3},{"title":"Recipes for building an open-domain chatbot","priority":1},{"title":"A Theory on Adam Instability in Large-Scale Machine Learning","priority":0},{"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models","priority":3},{"title":"The BELEBELE Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants","priority":0}],"keywords":["NLP","Language Models","BART","OPT (Meta)","LLaMA (Meta)","Chat Bots, Conversational AI","Platforms","Blender (Facebook)","Issues","Training Instability","Adam Instability","Llama 2","Communities, Networks","Social Media","Influencer","Propaganda","Z-Bloggers"]}