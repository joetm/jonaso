{"docs":[{"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","priority":1},{"title":"OPT: Open Pre-trained Transformer Language Models","priority":3},{"title":"LLaMA: Open and Efficient Foundation Language Models","priority":3},{"title":"Recipes for building an open-domain chatbot","priority":1},{"title":"A Theory on Adam Instability in Large-Scale Machine Learning","priority":0}],"keywords":["NLP","Language Models","BART","OPT (Meta)","LLaMA (Meta)","Chat Bots, Conversational AI","Platforms","Blender (Facebook)","Issues","Training Instability","Adam Instability"]}