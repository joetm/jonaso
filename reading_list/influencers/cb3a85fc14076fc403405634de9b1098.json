{"docs":[{"title":"Crowd Truth: Harnessing disagreement in crowdsourcing a relation extraction gold standard","priority":3},{"title":"Empirical Methodology for Crowdsourcing Ground Truth","priority":1},{"title":"Crowdsourcing Subjective Tasks:  The Case Study of Understanding Toxicity in Online Discussions","priority":0},{"title":"Semantic Web and Human Computation: the Status of an Emerging Field","priority":3},{"title":"Nichesourcing: Harnessing the Power of Crowds of Experts","priority":2},{"title":"Accurator: Nichesourcing for Cultural Heritage","priority":3},{"title":"Personalized nichesourcing","priority":1},{"title":"\u201cEveryone wants to do the model work, not the data work\u201d: Data Cascades in High-Stakes AI","priority":2},{"title":"The Rijksmuseum Collection as Linked Data","priority":2}],"keywords":["Crowdsourcing, Human Computation","Process, Workflow, Worker Selection, Task Design","CrowdTruth","Issues","Subjectivity, Subjective Tasks","Ontology Engineering","Crowdsourcing","General","Nichesourcing","Accurator","Machine Learning","Data Quality","Data Cascades","Museum","Rijksmuseum"]}