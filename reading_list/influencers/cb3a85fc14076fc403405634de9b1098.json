{"docs":[{"title":"Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection","priority":1},{"title":"Nichesourcing: Harnessing the Power of Crowds of Experts","priority":2},{"title":"Accurator: Nichesourcing for Cultural Heritage","priority":3},{"title":"Personalized nichesourcing","priority":1},{"title":"Crowdsourcing Subjective Tasks: The Case Study of Understanding Toxicity in Online Discussions","priority":0},{"title":"Crowdsourcing in the Cultural Heritage Domain: Opportunities and Challenges","priority":3},{"title":"Rapid Instance-Level Knowledge Acquisition for Google Maps from Class-Level Common Sense","priority":0},{"title":"Truth Is a Lie: Crowd Truth and the Seven Myths of Human Annotation","priority":3},{"title":"Crowd Truth: Harnessing disagreement in crowdsourcing a relation extraction gold standard","priority":3},{"title":"Empirical Methodology for Crowdsourcing Ground Truth","priority":1},{"title":"The Rijksmuseum Collection as Linked Data","priority":2},{"title":"\u201cEveryone wants to do the model work, not the data work\u201d: Data Cascades in High-Stakes AI","priority":2},{"title":"Semantic Web and Human Computation: the Status of an Emerging Field","priority":3},{"title":"CrowdTruth: Machine-Human Computation Framework for Harnessing Disagreement in Gathering Annotated Data","priority":2},{"title":"Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation","priority":2}],"keywords":["AI","Responsible AI","Crowdsourcing, Human Computation","General, Classification, Frameworks","Nichesourcing","Accurator","Issues","Subjectivity, Subjective Tasks","Applications","Cultural Heritage","Knowledge Bases","Knowledge Base Completion","Process, Workflow, Worker Selection","CrowdTruth","Museum","Rijksmuseum","Machine Learning","Data Quality","Data Cascades","Ontology Engineering","Crowdsourcing","General","Generative Deep Learning","Safety Filters","Adversarial Nibbler"]}