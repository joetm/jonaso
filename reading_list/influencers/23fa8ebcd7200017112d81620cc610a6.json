{"docs":[{"title":"Pretraining Language Models with Human Preferences","priority":2},{"title":"The Capacity for Moral Self-Correction in Large Language Models","priority":3},{"title":"Measuring Faithfulness in Chain-of-Thought Reasoning","priority":2},{"title":"Eight Things to Know about Large Language Models","priority":3},{"title":"TOWARDS UNDERSTANDING SYCOPHANCY IN LANGUAGE MODELS","priority":2},{"title":"Discovering Language Model Behaviors with Model-Written Evaluations","priority":2},{"title":"Constitutional AI: Harmlessness from AI Feedback","priority":3},{"title":"SLEEPER AGENTS: TRAINING DECEPTIVE LLMS","priority":1},{"title":"ALIGNMENT FAKING IN LARGE LANGUAGE MODELS","priority":2},{"title":"Sabotage Evaluations for Frontier Models","priority":1},{"title":"Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety","priority":3}],"keywords":["AI","Issues","Human-AI Alignment","Human Preferences","Prompt Engineering","Prompt Techniques","Moral Self-Correction","Chain of Thought, CoT","NLP","Sycophancy, Sandbagging","Scaling","Inverse Scaling","Model-Written Evaluations","Machine Learning","Reinforcement Learning","Reinforcement Learning with Human Feedback (RLHF)","Constitutional AI","Agents, Autonomous Task Management","Sleeper Agents","Alignment Faking","Sabotage","Prompt Engineering Techniques","Chain of Thought Monitorability"]}