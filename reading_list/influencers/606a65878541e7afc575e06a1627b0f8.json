{"docs":[{"title":"Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models","priority":0},{"title":"AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts","priority":1},{"title":"Universal Adversarial Triggers for Attacking and Analyzing NLP","priority":3},{"title":"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning","priority":2},{"title":"ART: Automatic multi-step reasoning and tool-use for large language models","priority":2},{"title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models","priority":2},{"title":"Anchors: High-Precision Model-Agnostic Explanations","priority":1},{"title":"WHAT\u2019S IN MY BIG DATA?","priority":3}],"keywords":["Prompt Engineering","NLP","Fine-Tuning","Few-Shot Finetuning","AutoPrompt","Machine Learning","Adversarial Machine Learning","Universal Adversarial Triggers","Issues","Long-tail Knowledge","Prompt Techniques","Automatic Reasoning and Tool-Use (ART)","Calibration","Contextual Calibration","AI","Explainable AI, XAI","Interpretability","Interpretability Tools","Anchors","Training","Training Data","What's in My Big Data (WIMBD)"]}