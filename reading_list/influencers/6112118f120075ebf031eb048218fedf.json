{"docs":[{"title":"Fast Inference from Transformers via Speculative Decoding","priority":2},{"title":"Face0: Instantaneously Conditioning a Text-to-Image Model on a Face","priority":3},{"title":"Crowdsourcing Dermatology Images with Google Search Ads: Creating a Real-World Skin Condition Dataset","priority":2},{"title":"SELECTIVE ATTENTION IMPROVES TRANSFORMER","priority":1},{"title":"Towards an AI co-scientist","priority":3}],"keywords":["NLP","Transformers","Speculative Decoding","Machine Learning","Generative Deep Learning","Text-to-Image","Face0","Benchmarks, gold standards, datasets","Images","Dermatology","SCIN","Attention","Selective Attention","Future Science","AI Research Assistants","AI Co-Scientist"]}