{"docs":[{"title":"Green AI","priority":1},{"title":"Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text","priority":0},{"title":"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping","priority":3},{"title":"WHAT\u2019S IN MY BIG DATA?","priority":3},{"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","priority":1},{"title":"Closed AI Models Make Bad Baselines","priority":1}],"keywords":["AI","Issues","Sustainability","Red AI, Green AI","Benchmarks, gold standards, datasets","Images","Multimodal C4 (MMC4)","NLP","Fine-Tuning","Random Seeds, Brittleness","Early Stopping","Training","Training Data","What's in My Big Data (WIMBD)","Language Models","BLOOM","Machine Learning","Evaluation","Closed Models Baselines"]}