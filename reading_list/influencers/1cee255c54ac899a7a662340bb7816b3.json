{"docs":[{"title":"Longformer: The Long-Document Transformer","priority":1},{"title":"SPECTER: Document-level Representation Learning using Citation-informed Transformers","priority":1},{"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","priority":1},{"title":"Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks","priority":1}],"keywords":["NLP","Language Models","Longformer","Specter","BLOOM","Machine Learning","General, Theory","Training"]}