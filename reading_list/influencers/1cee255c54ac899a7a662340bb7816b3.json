{"docs":[{"title":"Longformer: The Long-Document Transformer","priority":1},{"title":"SPECTER: Document-level Representation Learning using Citation-informed Transformers","priority":1},{"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","priority":1},{"title":"Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks","priority":1},{"title":"Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","priority":2}],"keywords":["NLP","Language Models","Longformer","Specter","BLOOM","Machine Learning","General, Theory","Training","Benchmarks, gold standards, datasets","Dolma"]}