{"docs":[{"title":"OPT: Open Pre-trained Transformer Language Models","priority":3},{"title":"A Theory on Adam Instability in Large-Scale Machine Learning","priority":0},{"title":"LIMA: Less Is More for Alignment","priority":2},{"title":"Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning","priority":1}],"keywords":["NLP","Language Models","OPT (Meta)","Issues","Training Instability","Adam Instability","AI","Human-AI Alignment","LIMA","Machine Learning","Generative Deep Learning","Multimodal Generation","CM3Leon"]}