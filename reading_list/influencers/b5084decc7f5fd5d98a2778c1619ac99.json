{"docs":[{"title":"OPT: Open Pre-trained Transformer Language Models","priority":3},{"title":"A Theory on Adam Instability in Large-Scale Machine Learning","priority":0},{"title":"LIMA: Less Is More for Alignment","priority":2}],"keywords":["NLP","Language Models","OPT (Meta)","Issues","Training Instability","Adam Instability","AI","Human-AI Alignment","LIMA"]}