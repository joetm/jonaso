{"docs":[{"title":"WHAT\u2019S IN MY BIG DATA?","priority":3},{"title":"Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","priority":2},{"title":"OLMo : Accelerating the Science of Language Models","priority":1},{"title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus","priority":3},{"title":"OLMoE: Open Mixture-of-Experts Language Models","priority":1},{"title":"Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models","priority":1}],"keywords":["NLP","Training","Training Data","What's in My Big Data (WIMBD)","Benchmarks, gold standards, datasets","Dolma","Language Models","OLMo (AI2)","Colossal Clean Crawled Corpus (C4)","Fine-Tuning","Merging, Blending","Mixture of Experts","Open Mixture-of-Experts (OlMoE)","Machine Learning","Generative Deep Learning","Multimodal Generation","Molmo, PixMo"]}