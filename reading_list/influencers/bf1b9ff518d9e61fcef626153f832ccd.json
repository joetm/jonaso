{"docs":[{"title":"LARGE LANGUAGE MODELS","priority":3},{"title":"LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS","priority":1},{"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models","priority":3},{"title":"SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS","priority":2},{"title":"Large Language Models Can Be Easily Distracted by Irrelevant Context","priority":2},{"title":"A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity","priority":2},{"title":"UL2: Unifying Language Learning Paradigms","priority":2},{"title":"Emergent Abilities of Large Language Models","priority":3},{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1},{"title":"Scaling Instruction-Finetuned Language Models","priority":1},{"title":"Learning from the Wisdom of Crowds by Minimax Entropy","priority":0},{"title":"RECITATION-AUGMENTED LANGUAGE MODELS","priority":3}],"keywords":["Prompt Engineering","Applications","Optimization","Optimization by Prompting (OPRO)","Prompt Techniques","Least-to-most Prompting","Chain of Thought, CoT","Self-Consistency, CoT-SC","NLP","Issues","Irrelevant Context","Grade-School Math with Irrelevant Context (GSM-IC)","Training","Unified Language Learning","UL2 (Google)","General, Theory","Emergent Abilities","Language Models","PaLM (Google)","Flan-T5","Instruction Finetuning","Collective Intelligence","Wisdom of the Crowd","Entropy","Minimax Entropy","Augmented Language Models","Recitation-Augmented LLMs"]}