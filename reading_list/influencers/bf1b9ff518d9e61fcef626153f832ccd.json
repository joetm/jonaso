{"docs":[{"title":"LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS","priority":1},{"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models","priority":2},{"title":"SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS","priority":1},{"title":"Large Language Models Can Be Easily Distracted by Irrelevant Context","priority":2},{"title":"UL2: Unifying Language Learning Paradigms","priority":2},{"title":"Emergent Abilities of Large Language Models","priority":2},{"title":"Scaling Instruction-Finetuned Language Models","priority":1},{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1},{"title":"Learning from the Wisdom of Crowds by Minimax Entropy","priority":0},{"title":"A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity","priority":2},{"title":"LARGE LANGUAGE MODELS","priority":3}],"keywords":["Prompt Engineering","Prompt Techniques","Least-to-most Prompting","Chain of Thought Prompting","Self-Consistency","NLP","Issues","Irrelevant Context","Grade-School Math with Irrelevant Context (GSM-IC)","Training","Unified Language Learning","UL2 (Google)","General, Theory","Emergent Abilities","Language Models","Flan-T5","Instruction Finetuning","Google PaLM","Collective Intelligence","Wisdom of the Crowd","Entropy","Minimax Entropy","Applications","Optimization","Optimization by Prompting (OPRO)"]}