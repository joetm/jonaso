{"docs":[{"title":"Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models","priority":0},{"title":"AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts","priority":1},{"title":"Universal Adversarial Triggers for Attacking and Analyzing NLP","priority":3},{"title":"Extracting Training Data from Large Language Models","priority":1},{"title":"Extracting Training Data from Diffusion Models","priority":3},{"title":"The False Promise of Imitating Proprietary LLMs","priority":3}],"keywords":["Prompt Engineering","NLP","Fine-Tuning","Few-Shot Finetuning","AutoPrompt","Machine Learning","Adversarial Machine Learning","Universal Adversarial Triggers","Issues","Private Training Data","Training Data Extraction Attack","Generative Deep Learning","Diffusion","Memorization of Training Data","Knowledge","Knowledge Distillation","Model Imitation","Performance Discrepancies"]}