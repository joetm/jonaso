{"docs":[{"title":"Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models","priority":0},{"title":"The False Promise of Imitating Proprietary LLMs","priority":3},{"title":"AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts","priority":1},{"title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models","priority":2},{"title":"Universal Adversarial Triggers for Attacking and Analyzing NLP","priority":3},{"title":"Extracting Training Data from Large Language Models","priority":1},{"title":"Extracting Training Data from Diffusion Models","priority":3},{"title":"Scalable Extraction of Training Data from (Production) Language Models","priority":3},{"title":"Stealing Part of a Production Language Model","priority":1},{"title":"Deliberative Alignment: Reasoning Enables Safer Language Models","priority":2}],"keywords":["Prompt Engineering","Knowledge","Knowledge Distillation","Model Imitation","Performance Discrepancies","NLP","Fine-Tuning","Few-Shot Finetuning","AutoPrompt","Issues","Calibration","Contextual Calibration","Machine Learning","Adversarial Machine Learning","Universal Adversarial Triggers","Private Training Data","Training Data Extraction Attack","Generative Deep Learning","Diffusion","Memorization of Training Data","Security Vulnerabilities","Training Data Extraction","Extractable Memorization","Security","Model-Stealing Attacks","Last Layer","AI","Human-AI Alignment","Deliberative Alignment"]}