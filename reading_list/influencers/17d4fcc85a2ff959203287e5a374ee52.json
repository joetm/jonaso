{"docs":[{"title":"TLDR: Extreme Summarization of Scientific Documents","priority":2},{"title":"Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks","priority":1},{"title":"Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","priority":2},{"title":"OLMo : Accelerating the Science of Language Models","priority":1}],"keywords":["NLP","Natural Language Generation, NLG","Summarization","Paper Summarization","TLDR","Machine Learning","General, Theory","Training","Benchmarks, gold standards, datasets","Dolma","Language Models","OLMo (AI2)"]}