{"docs":[{"title":"TLDR: Extreme Summarization of Scientific Documents","priority":2},{"title":"Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks","priority":1},{"title":"Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","priority":2},{"title":"OLMo : Accelerating the Science of Language Models","priority":1},{"title":"The Foundation Model Development Cheatsheet","priority":3},{"title":"OLMoE: Open Mixture-of-Experts Language Models","priority":1},{"title":"Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models","priority":1},{"title":"S2ORC: The Semantic Scholar Open Research Corpus","priority":3}],"keywords":["NLP","Natural Language Generation, NLG","Summarization","Paper Summarization","TLDR","Machine Learning","General, Theory","Training","Benchmarks, gold standards, datasets","Dolma","Language Models","OLMo (AI2)","Foundation Model Development Cheatsheet","Fine-Tuning","Merging, Blending","Mixture of Experts","Open Mixture-of-Experts (OlMoE)","Generative Deep Learning","Multimodal Generation","Molmo, PixMo","Science","Semantic Scholar Open Research Corpus (S2ORC)"]}