{"docs":[{"title":"LIMA: Less Is More for Alignment","priority":2},{"title":"Self-Alignment with Instruction Backtranslation","priority":3},{"title":"Hierarchical Neural Story Generation","priority":0},{"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?","priority":2},{"title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks","priority":1},{"title":"Retrieval-augmented generation (\u201cRAG\u201d)","priority":3},{"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","priority":1},{"title":"MEASURING AND NARROWING THE COMPOSITIONALITY GAP IN LANGUAGE MODELS","priority":2},{"title":"TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION","priority":1}],"keywords":["AI","Issues","Human-AI Alignment","LIMA","Self-Alignment","Instruction Backtranslation","NLP","Natural Language Generation, NLG","Applications","Story Generation","In-Context Learning","Meaning","Augmented Language Models","Retireval Augmentation","Retrieval-Augmented Generation (RAG)","Huggingface RAG","Language Models","BART","Prompt Engineering","Prompt Techniques","Self-Ask Prompt","Compositionality Gap","Training","ALiBi"]}