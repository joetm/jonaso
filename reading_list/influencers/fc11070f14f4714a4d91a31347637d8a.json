{"docs":[{"title":"Hierarchical Neural Story Generation","priority":0},{"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","priority":1},{"title":"LIMA: Less Is More for Alignment","priority":2},{"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?","priority":2}],"keywords":["NLP","Natural Language Generation, NLG","Applications","Story Generation","Language Models","BART","AI","Issues","Human-AI Alignment","LIMA","In-Context Learning","Meaning"]}