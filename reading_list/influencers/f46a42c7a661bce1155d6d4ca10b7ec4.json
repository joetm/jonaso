{"docs":[{"title":"Cut the CARP: Fishing for zero-shot story evaluation","priority":1},{"title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling","priority":2},{"title":"Simulators","priority":0},{"title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","priority":1},{"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","priority":1},{"title":"WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION","priority":3}],"keywords":["Hypertext, Hypermedia","Narrative, Story, Fabula","Story Evaluation","Contrastive Authoring and Reviewing Pairing (CARP)","Benchmarks, gold standards, datasets","NLP","The Pile","Applications","Simulation","Simulators","Language Models","GPT","GPT-Neo","GPT-NeoX-20B","BLOOM","AI","Impact, Future","Superalignment","Weak-to-strong Generalization"]}