{"docs":[{"title":"How Can We Know What Language Models Know?","priority":3},{"title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","priority":3},{"title":"WebArena: A Realistic Web Environment for Building Autonomous Agents","priority":2},{"title":"An Unsupervised Model for Joint Phrase Alignment and Extraction","priority":0},{"title":"Cross-Modal Fine-Tuning: Align then Refine","priority":1},{"title":"Repetition Improves Language Model Embeddings","priority":1},{"title":"Learning to Filter Context for Retrieval-Augmented Generation","priority":2},{"title":"Training Software Engineering Agents and Verifiers with SWE-Gym","priority":2}],"keywords":["Prompt Engineering","Automated Prompts","General, Theory","Benchmarks, gold standards, datasets","NLP","Agents","WebArena","Phrase Detection, Phrase Extraction","Machine Learning","Fine-Tuning","Cross-modal Fine-tuning","Issues","Repetition Problem","Echo Embeddings","Augmented Language Models","Retrieval-Augmented Generation (RAG)","RAG Optimization, RAG Fine-Tuning, RAG Techniques","Context Filtering","FilCo","Agents, Autonomous Task Management","Applications","Software Development","SWE-Gym"]}