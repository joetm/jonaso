{"docs":[{"title":"Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks","priority":1},{"title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus","priority":3}],"keywords":["Machine Learning","General, Theory","Training","Benchmarks, gold standards, datasets","NLP","Colossal Clean Crawled Corpus (C4)"]}