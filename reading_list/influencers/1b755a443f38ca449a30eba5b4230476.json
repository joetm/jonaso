{"docs":[{"title":"Foundation Models for Decision Making: Problems, Methods, and Opportunities","priority":0},{"title":"LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS","priority":1},{"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models","priority":3},{"title":"SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS","priority":2},{"title":"B E Y O N D T H E I M I TAT I O N G A M E : Q U A N T I F Y- I N G A N D E X T R A P O L AT I N G T H E C A PA B I L I T I E S O F L A N G U A G E M O D E L S","priority":1},{"title":"FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS","priority":1},{"title":"A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity","priority":2},{"title":"UL2: Unifying Language Learning Paradigms","priority":2},{"title":"EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks","priority":1},{"title":"Emergent Abilities of Large Language Models","priority":3},{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1},{"title":"Scaling Instruction-Finetuned Language Models","priority":1}],"keywords":["AI","Foundation Models","Applications","Decision Making","Prompt Engineering","Prompt Techniques","Least-to-most Prompting","Chain of Thought, CoT","Self-Consistency, CoT-SC","Benchmarks, gold standards, datasets","NLP","Beyond the Imitation Game (BIG-Bench)","Fine-Tuning","Instruction Tuning","Training","Unified Language Learning","UL2 (Google)","Data Augmentation","EDA, Easy Data Augmentation Techniques","General, Theory","Emergent Abilities","Language Models","PaLM (Google)","Flan-T5","Instruction Finetuning"]}