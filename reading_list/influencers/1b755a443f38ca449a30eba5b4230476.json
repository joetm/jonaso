{"docs":[{"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models","priority":1},{"title":"FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS","priority":1},{"title":"UL2: Unifying Language Learning Paradigms","priority":2},{"title":"Emergent Abilities of Large Language Models","priority":2},{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1},{"title":"EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks","priority":1},{"title":"Scaling Instruction-Finetuned Language Models","priority":1},{"title":"SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS","priority":1},{"title":"LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS","priority":1},{"title":"Foundation Models for Decision Making: Problems, Methods, and Opportunities","priority":0}],"keywords":["Prompt Engineering","Prompt Techniques","Chain of Thought Prompting","NLP","Fine-Tuning","Instruction Tuning","Training","Unified Language Learning","UL2 (Google)","General, Theory","Emergent Abilities","Language Models","Google PaLM","Data Augmentation","EDA, Easy Data Augmentation Techniques","Flan-T5","Instruction Finetuning","Self-Consistency","Least-to-most Prompting","AI","Foundation Models","Applications","Decision Making"]}