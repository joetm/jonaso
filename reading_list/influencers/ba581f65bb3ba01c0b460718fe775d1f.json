{"docs":[{"title":"Extracting Training Data from Large Language Models","priority":1},{"title":"Scaling Laws for Autoregressive Generative Modeling","priority":0},{"title":"Language Models are Few-Shot Learners","priority":3},{"title":"Measuring the Algorithmic Efficiency of Neural Networks","priority":1},{"title":"Predictability and Surprise in Large Generative Models","priority":2}],"keywords":["Machine Learning","Issues","Private Training Data","Training Data Extraction Attack","NLP","Language Models","GPT","Scaling Laws","GPT-3","Training Efficiency","AI","Predictability and Surprise"]}