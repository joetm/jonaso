{"docs":[{"title":"OPT: Open Pre-trained Transformer Language Models","priority":3},{"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?","priority":2},{"title":"The BELEBELE Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants","priority":0}],"keywords":["NLP","Language Models","OPT (Meta)","In-Context Learning","Meaning","Communities, Networks","Social Media","Influencer","Propaganda","Z-Bloggers"]}