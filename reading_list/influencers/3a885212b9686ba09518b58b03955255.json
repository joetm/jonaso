{"docs":[{"title":"The BELEBELE Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants","priority":0},{"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?","priority":2},{"title":"OPT: Open Pre-trained Transformer Language Models","priority":3}],"keywords":["Communities, Networks","Social Media","Influencer","Propaganda","Z-Bloggers","NLP","In-Context Learning","Meaning","Language Models","OPT (Meta)"]}