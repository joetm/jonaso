{"docs":[{"title":"The Foundation Model Transparency Index","priority":3},{"title":"A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity","priority":2},{"title":"Scaling Instruction-Finetuned Language Models","priority":1},{"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","priority":1}],"keywords":["AI","Issues","Transparency","Foundation Model Transparency Index","NLP","Training","Language Models","Flan-T5","Instruction Finetuning","BLOOM"]}