{"docs":[{"title":"Scaling Instruction-Finetuned Language Models","priority":1},{"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","priority":1},{"title":"A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity","priority":2},{"title":"The Foundation Model Transparency Index","priority":3}],"keywords":["NLP","Language Models","Flan-T5","Instruction Finetuning","BLOOM","Training","AI","Issues","Transparency","Foundation Model Transparency Index"]}