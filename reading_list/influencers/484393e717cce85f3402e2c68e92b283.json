{"docs":[{"title":"Deep contextualized word representations","priority":1},{"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","priority":1},{"title":"OPT: Open Pre-trained Transformer Language Models","priority":3},{"title":"Toolformer: Language Models Can Teach Themselves to Use Tools","priority":3},{"title":"LIMA: Less Is More for Alignment","priority":2},{"title":"QLORA: Efficient Finetuning of Quantized LLMs","priority":2},{"title":"ART: Automatic multi-step reasoning and tool-use for large language models","priority":2},{"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?","priority":2},{"title":"Self-Alignment with Instruction Backtranslation","priority":3}],"keywords":["NLP","Word Embeddings","Embeddings from Language Models (ELMo)","Language Models","BART","OPT (Meta)","External Knowledge Bases","Toolformer","AI","Issues","Human-AI Alignment","LIMA","Machine Learning","Generative Deep Learning","Text-to-Image","Stable Diffusion","Fine-tuning Stable Diffusion","Low-Rank Adaptation, LoRA","qLoRA","Prompt Engineering","Prompt Techniques","Automatic Reasoning and Tool-Use (ART)","In-Context Learning","Meaning","Self-Alignment","Instruction Backtranslation"]}