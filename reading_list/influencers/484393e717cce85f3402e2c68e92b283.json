{"docs":[{"title":"Deep contextualized word representations","priority":1},{"title":"OPT: Open Pre-trained Transformer Language Models","priority":3},{"title":"Toolformer: Language Models Can Teach Themselves to Use Tools","priority":3},{"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","priority":1}],"keywords":["NLP","Word Embeddings","Embeddings from Language Models (ELMo)","Language Models","OPT (Meta)","External Knowledge Bases","Toolformer","BART"]}