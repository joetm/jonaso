{"docs":[{"title":"LIMA: Less Is More for Alignment","priority":2},{"title":"Self-Alignment with Instruction Backtranslation","priority":3},{"title":"ART: Automatic multi-step reasoning and tool-use for large language models","priority":2},{"title":"The BELEBELE Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants","priority":0},{"title":"Shepherd: A Critic for Language Model Generation","priority":1},{"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?","priority":2},{"title":"Deep contextualized word representations","priority":1},{"title":"One Embedder, Any Task: Instruction-Finetuned Text Embeddings","priority":3},{"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","priority":1},{"title":"OPT: Open Pre-trained Transformer Language Models","priority":3},{"title":"Toolformer: Language Models Can Teach Themselves to Use Tools","priority":3},{"title":"QLORA: Efficient Finetuning of Quantized LLMs","priority":2},{"title":"Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning","priority":1},{"title":"DEMYSTIFYING CLIP DATA","priority":3},{"title":"AllenNLP: A Deep Semantic Natural Language Processing Platform","priority":1},{"title":"Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","priority":2}],"keywords":["AI","Issues","Human-AI Alignment","LIMA","Self-Alignment","Instruction Backtranslation","Prompt Engineering","Prompt Techniques","Automatic Reasoning and Tool-Use (ART)","Communities, Networks","Social Media","Influencer","Propaganda","Z-Bloggers","NLP","Natural Language Generation, NLG","Evaluation","Shepherd","In-Context Learning","Meaning","Word Embeddings","Embeddings from Language Models (ELMo)","Embeddings","Instructor (Instruction-based Omnifarious Representations)","Language Models","BART","OPT (Meta)","External Knowledge Bases","Toolformer (Meta)","Machine Learning","Generative Deep Learning","Text-to-Image","Stable Diffusion","Fine-tuning Stable Diffusion","Low-Rank Adaptation, LoRA","qLoRA","Multimodal Generation","CM3Leon","Contrastive Learning, Multimodal Models","CLIP","MetaCLIP,  Metadata-Curated Language-Image Pre-training","Tools, NLP APIs","AllenNLP","Benchmarks, gold standards, datasets","Dolma"]}