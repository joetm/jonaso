{"docs":[{"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model","priority":2},{"title":"Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents","priority":1}],"keywords":["Machine Learning","Reinforcement Learning","Reinforcement Learning with Human Feedback (RLHF)","Direct Preference Optimization (DPO)","NLP","Reasoning","Agent Q"]}