{"docs":[{"title":"SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS","priority":1},{"title":"Scaling Instruction-Finetuned Language Models","priority":1},{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1},{"title":"Character-Aware Models Improve Visual Text Rendering","priority":0},{"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","priority":2},{"title":"A Theory on Adam Instability in Large-Scale Machine Learning","priority":0}],"keywords":["Prompt Engineering","Prompt Techniques","Chain of Thought Prompting","Self-Consistency","NLP","Language Models","Flan-T5","Instruction Finetuning","Google PaLM","Machine Learning","Generative Deep Learning","Text-to-Image","Issues","Spelling","Benchmarks, gold standards, datasets","Colossal Clean Crawled Corpus (C4)","Training Instability","Adam Instability"]}