{"docs":[{"title":"SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS","priority":2},{"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","priority":2},{"title":"A Theory on Adam Instability in Large-Scale Machine Learning","priority":0},{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1},{"title":"Scaling Instruction-Finetuned Language Models","priority":1},{"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models","priority":3},{"title":"Character-Aware Models Improve Visual Text Rendering","priority":0},{"title":"Law of theWeakest Link: Cross Capabilities of Large LanguageModels","priority":3}],"keywords":["Prompt Engineering","Prompt Techniques","Chain of Thought, CoT","Self-Consistency, CoT-SC","Benchmarks, gold standards, datasets","NLP","Colossal Clean Crawled Corpus (C4)","Issues","Training Instability","Adam Instability","Language Models","PaLM (Google)","Flan-T5","Instruction Finetuning","LLaMA (Meta)","Llama 2","Machine Learning","Generative Deep Learning","Text-to-Image","Spelling","General, Theory","Emergent Abilities","Cross Capabilities","Law of the Weakest Link, CrossEval"]}