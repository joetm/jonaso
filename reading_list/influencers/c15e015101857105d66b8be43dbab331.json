{"docs":[{"title":"SAM-CLIP: MERGING VISION FOUNDATION MODELS TOWARDS SEMANTIC AND SPATIAL UNDERSTANDING","priority":1},{"title":"Weight Subcloning: Direct Initialization of Transformers Using Larger Pretrained Ones","priority":1},{"title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory","priority":1},{"title":"The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity","priority":3}],"keywords":["Machine Learning","Contrastive Learning, Multimodal Models","CLIP","SAM-CLIP","NLP","Transformers","Weight Subcloning","Inference","Memory","Constrained Memory","Reasoning","Issues"]}