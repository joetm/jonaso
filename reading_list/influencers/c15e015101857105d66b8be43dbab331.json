{"docs":[{"title":"SAM-CLIP: MERGING VISION FOUNDATION MODELS TOWARDS SEMANTIC AND SPATIAL UNDERSTANDING","priority":1},{"title":"Weight Subcloning: Direct Initialization of Transformers Using Larger Pretrained Ones","priority":1},{"title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory","priority":1}],"keywords":["Machine Learning","Contrastive Learning, Multimodal Models","CLIP","SAM-CLIP","NLP","Transformers","Weight Subcloning","Inference","Memory","Constrained Memory"]}