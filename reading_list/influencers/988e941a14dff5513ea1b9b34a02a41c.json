{"docs":[{"title":"Energy and Policy Considerations for Deep Learning in NLP","priority":1},{"title":"Closed AI Models Make Bad Baselines","priority":1},{"title":"Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","priority":2},{"title":"OLMo : Accelerating the Science of Language Models","priority":1},{"title":"Energy and Policy Considerations for Modern Deep Learning Research","priority":2},{"title":"Power Hungry Processing: Watts Driving the Cost of AI Deployment?","priority":1}],"keywords":["Machine Learning","Issues","Carbon Footprint, Sustainability, Green Computing","Evaluation","Closed Models Baselines","Benchmarks, gold standards, datasets","NLP","Dolma","Language Models","OLMo (AI2)","Generative Deep Learning","Energy Consumption"]}