{"docs":[{"title":"Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery","priority":3},{"title":"Universal Guidance for Diffusion Models","priority":1},{"title":"Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise","priority":2},{"title":"A Cookbook of Self-Supervised Learning","priority":2},{"title":"Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text","priority":1},{"title":"Understanding and Mitigating Copying in Diffusion Models","priority":2},{"title":"Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models","priority":2}],"keywords":["Prompt Engineering","Prompt Learning","Soft Prompts","Prompt Tuning","Hard Prompts Made Easy (PEZ)","Machine Learning","Generative Deep Learning","Guidance","Universal Guidance","Diffusion","Deep Learning","Self Supervised Learning","NLP","Natural Language Generation, NLG","Detecting AI-Generated Text","Binoculars","Issues","Data Replication, Memorization of Training Data"]}