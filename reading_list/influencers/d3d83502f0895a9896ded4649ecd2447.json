{"docs":[{"title":"Predictability and Surprise in Large Generative Models","priority":2},{"title":"The Capacity for Moral Self-Correction in Large Language Models","priority":3},{"title":"Discovering Language Model Behaviors with Model-Written Evaluations","priority":2},{"title":"Measuring the Algorithmic Efficiency of Neural Networks","priority":1},{"title":"Constitutional AI: Harmlessness from AI Feedback","priority":3},{"title":"In-context Learning and Induction Heads","priority":2},{"title":"Measuring Faithfulness in Chain-of-Thought Reasoning","priority":2}],"keywords":["AI","Issues","Predictability and Surprise","Prompt Engineering","Prompt Techniques","Moral Self-Correction","NLP","Scaling","Inverse Scaling","Model-Written Evaluations","Language Models","GPT","GPT-3","Training Efficiency","Machine Learning","Reinforcement Learning","Reinforcement Learning with Human Feedback (RLHF)","Constitutional AI","In-Context Learning","Induction Heads","Chain of Thought, CoT"]}