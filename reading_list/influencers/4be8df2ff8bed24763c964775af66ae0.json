{"docs":[{"title":"How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs","priority":2}],"keywords":["NLP","Issues","Adversarial Attacks"]}