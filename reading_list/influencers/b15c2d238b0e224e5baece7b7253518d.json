{"docs":[{"title":"Creative Writing with a Machine in the Loop: Case Studies on Slogans and Stories","priority":2},{"title":"Explain like I am a Scientist: The Linguistic Barriers of Entry to r\/science","priority":1},{"title":"SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions","priority":3},{"title":"Generating Scientific Definitions with Controllable Complexity","priority":0},{"title":"Framing Effect: Choice of Slogans Used to Advertise Online Experiments Can Boost Recruitment and Lead to Sample Biases","priority":2},{"title":"Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks","priority":1},{"title":"Closed AI Models Make Bad Baselines","priority":1},{"title":"Green AI","priority":1},{"title":"How Language Model Hallucinations","priority":2},{"title":"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping","priority":3},{"title":"WHAT\u2019S IN MY BIG DATA?","priority":3},{"title":"One Embedder, Any Task: Instruction-Finetuned Text Embeddings","priority":3}],"keywords":["Creativity","Creativity Support Tools","Applications","Storytelling","Machine-in-the-loop creative writing","Communities, Networks","Issues","Homophily","Language","NLP","Fine-Tuning","Instruction Tuning","Self-Instruct","Definitions","Experimentation","Framing","Machine Learning","General, Theory","Training","Evaluation","Closed Models Baselines","AI","Sustainability","Red AI, Green AI","Hallucination, Factual Information","Hallucination Snowballing","Random Seeds, Brittleness","Early Stopping","Training Data","What's in My Big Data (WIMBD)","Embeddings","Instructor (Instruction-based Omnifarious Representations)"]}