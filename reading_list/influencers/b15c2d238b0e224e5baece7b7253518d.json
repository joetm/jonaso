{"docs":[{"title":"Green AI","priority":1},{"title":"Creative Writing with a Machine in the Loop: Case Studies on Slogans and Stories","priority":2},{"title":"Explain like I am a Scientist: The Linguistic Barriers of Entry to r\/science","priority":1},{"title":"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping","priority":3},{"title":"SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions","priority":3},{"title":"How Language Model Hallucinations","priority":2},{"title":"WHAT\u2019S IN MY BIG DATA?","priority":3},{"title":"Generating Scientific Definitions with Controllable Complexity","priority":0},{"title":"One Embedder, Any Task: Instruction-Finetuned Text Embeddings","priority":3},{"title":"Framing Effect: Choice of Slogans Used to Advertise Online Experiments Can Boost Recruitment and Lead to Sample Biases","priority":2},{"title":"Closed AI Models Make Bad Baselines","priority":1},{"title":"Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks","priority":1},{"title":"MEASURING AND NARROWING THE COMPOSITIONALITY GAP IN LANGUAGE MODELS","priority":2},{"title":"TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION","priority":1},{"title":"Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","priority":2},{"title":"OLMo : Accelerating the Science of Language Models","priority":1}],"keywords":["AI","Issues","Sustainability","Red AI, Green AI","Creativity","Creativity Support Tools","Applications","Storytelling","Machine-in-the-loop creative writing","Communities, Networks","Homophily","Language","NLP","Fine-Tuning","Random Seeds, Brittleness","Early Stopping","Instruction Tuning","Self-Instruct","Hallucination, Factual Information","Hallucination Snowballing","Training","Training Data","What's in My Big Data (WIMBD)","Definitions","Embeddings","Instructor (Instruction-based Omnifarious Representations)","Experimentation","Framing","Machine Learning","Evaluation","Closed Models Baselines","General, Theory","Prompt Engineering","Prompt Techniques","Self-Ask Prompt","Compositionality Gap","ALiBi","Benchmarks, gold standards, datasets","Dolma","Language Models","OLMo (AI2)"]}