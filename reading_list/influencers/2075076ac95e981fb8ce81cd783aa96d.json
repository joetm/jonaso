{"docs":[{"title":"Crowd-based Multi-Predicate Screening of Papers in Literature Reviews","priority":1},{"title":"On the state of reporting in crowdsourcing experiments and a checklist to aid current practices","priority":0},{"title":"Issues and Directions","priority":1},{"title":"Quality Control in Crowdsourcing: A Survey of Quality Attributes, Assessment Techniques, and Assurance Actions","priority":3},{"title":"Quality Control in Crowdsourcing: A Survey of Quality Attributes, Assessment Techniques and Assurance Actions","priority":2}],"keywords":["Crowdsourcing, Human Computation","Applications","Research","Literature review, paper screening","Best Practices","Reporting Crowdsourcing Experiments","Checklist for Reporting Crowdsourcing Experiments","Issues","Quality, Accuracy"]}