{"docs":[{"title":"On the Opportunities and Risks of Foundation Models","priority":3},{"title":"Holistic Evaluation of Language Models","priority":2},{"title":"Foundations of Statistical Natural Language Processing","priority":0},{"title":"Answering Complex Open-domain Questions Through Iterative Query Generation","priority":2},{"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model","priority":2},{"title":"ReFT: Representation Finetuning for Language Models","priority":3},{"title":"Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools","priority":3},{"title":"Studying the History of Ideas Using Topic Models","priority":3},{"title":"VERBALIZED SAMPLING: HOW TO MITIGATE MODE COLLAPSE AND UNLOCK LLM DIVERSITY","priority":2}],"keywords":["AI","Foundation Models","NLP","Evaluation","Holistic Evaluation of Language Models (HELM)","General, Theory","Augmented Language Models","Retrieval-Augmented Generation (RAG)","Multi-hop Reasoning","GoldEn","Machine Learning","Reinforcement Learning","Reinforcement Learning with Human Feedback (RLHF)","Direct Preference Optimization (DPO)","Fine-Tuning","Representation Finetuning (ReFT)","Issues","Hallucination","Meta-Research","Research Topics","Generative Deep Learning","Mode Collapse","Verbalized Sampling (VS)"]}