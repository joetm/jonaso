{"docs":[{"title":"Measuring Faithfulness in Chain-of-Thought Reasoning","priority":2},{"title":"Discovering Language Model Behaviors with Model-Written Evaluations","priority":2},{"title":"Simulators","priority":0},{"title":"SLEEPER AGENTS: TRAINING DECEPTIVE LLMS","priority":1},{"title":"Engineering Monosemanticity in Toy Models","priority":1},{"title":"Risks from Learned Optimization\ufffd Introduction","priority":1},{"title":"ALIGNMENT FAKING IN LARGE LANGUAGE MODELS","priority":2},{"title":"Sabotage Evaluations for Frontier Models","priority":1}],"keywords":["Prompt Engineering","Prompt Techniques","Chain of Thought, CoT","NLP","Issues","Scaling","Inverse Scaling","Model-Written Evaluations","Applications","Simulation","Simulators","Agents, Autonomous Task Management","Sleeper Agents","General, Theory","Monosemanticity","AI","Human-AI Alignment","Mesa-Optimization","Alignment Faking","Sabotage"]}