{"docs":[{"title":"TORCHSCALE: Transformers at Scale","priority":1},{"title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","priority":2}],"keywords":["NLP","Language Models","Foundation Architectures","TorchScale","Optimization","Quantization","1-bit LLMs","BitNet","BitNet b1.58"]}