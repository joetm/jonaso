{"docs":[{"title":"VOYAGER: An Open-Ended Embodied Agent with Large Language Models","priority":3},{"title":"Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study","priority":0},{"title":"EUREKA: HUMAN-LEVEL REWARD DESIGN VIA CODING LARGE LANGUAGE MODELS","priority":1},{"title":"GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection","priority":2}],"keywords":["AI","Human-AI-Interaction, Human-Machine-Interaction","Embodied Agents","Mincraft Voyager","NLP","Augmented Language Models","Retrieval Pretraining","Machine Learning","Reinforcement Learning","Reward Design","Evolution-driven Universal Reward Kit for Agent (EUREKA)","Fine-Tuning","Low-Rank Adaptation (LoRA)","GaLore"]}