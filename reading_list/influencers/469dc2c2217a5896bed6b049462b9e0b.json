{"docs":[{"title":"AllenNLP: A Deep Semantic Natural Language Processing Platform","priority":1},{"title":"Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","priority":2},{"title":"OLMo : Accelerating the Science of Language Models","priority":1},{"title":"OLMoE: Open Mixture-of-Experts Language Models","priority":1}],"keywords":["NLP","Tools, NLP APIs","AllenNLP","Benchmarks, gold standards, datasets","Dolma","Language Models","OLMo (AI2)","Fine-Tuning","Merging, Blending","Mixture of Experts","Open Mixture-of-Experts (OlMoE)"]}