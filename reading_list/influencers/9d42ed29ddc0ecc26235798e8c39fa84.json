{"docs":[{"title":"Mechanical Turk is Not Anonymous","priority":2},{"title":"Probabilistic Modeling for Crowdsourcing Partially-Subjective Ratings","priority":0},{"title":"On Quality Control and Machine Learning in Crowdsourcing","priority":0},{"title":"Crowdsourcing for Search and Data Mining","priority":0},{"title":"Fast, Accurate, and Healthier: Interactive Blurring Helps Moderators Reduce Exposure to Harmful Content","priority":2},{"title":"Crowdsourcing Information Extraction for Biomedical Systematic Reviews","priority":0},{"title":"Beyond Mechanical Turk: An Analysis of Paid Crowd Work Platforms","priority":3},{"title":"Design Activism for Minimum Wage Crowd Work","priority":2},{"title":"Why Is That Relevant? Collecting Annotator Rationales for Relevance Judgments","priority":2},{"title":"The Psychological Well-Being of Content Moderators The Emotional Labor of Commercial Moderation and Avenues for Improving Support","priority":3},{"title":"The Future of Crowd Work","priority":3},{"title":"How Crowd Worker Factors Influence Subjective Annotations: A Study of Tagging Misogynistic Hate Speech in Tweets","priority":1}],"keywords":["Crowdsourcing, Human Computation","Issues","Anonymity, Invisibility","MTurk Anonymity","Subjectivity, Subjective Tasks","Quality, Accuracy","Quality Control","Applications","Search, Querying, Human Flesh Engine, Search Support","Moderation","Blurring","Information Extraction","Comparisons","Pricing","Design Activism","Task Design","Rationales","Communities, Networks","Future of Work","Crowdsourcing, Microtasks","Bias","Annotating Misogynistic Hate Speech"]}