{"docs":[{"title":"A Cookbook of Self-Supervised Learning","priority":2},{"title":"GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection","priority":2},{"title":"H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models","priority":2},{"title":"META-REWARDING LANGUAGE MODELS: Self-Improving Alignment with LLM-as-a-Meta-Judge","priority":3}],"keywords":["Machine Learning","Deep Learning","Self Supervised Learning","NLP","Fine-Tuning","Low-Rank Adaptation (LoRA)","GaLore","Inference","Heavy-Hitter Oracle (H2O)","Aligning Language Models","Self-Rewarding Language Models","Self-Improving Alignment","LLM-as-a-Meta-Judge"]}