{"docs":[{"title":"PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts","priority":2},{"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","priority":2},{"title":"Large Language Models Struggle to Learn Long-Tail Knowledge","priority":2},{"title":"Emergent Abilities of Large Language Models","priority":3},{"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","priority":1},{"title":"A Call to Build Models Like We Build Open- Source Software","priority":2},{"title":"A call to build models like we build open-source software","priority":0},{"title":"Building Machine Learning Models Like Open Source Software","priority":2},{"title":"Extracting Training Data from Large Language Models","priority":1},{"title":"Scaling Data-Constrained Language Models","priority":3},{"title":"A Survey on Data Selection for Language Models","priority":3}],"keywords":["Prompt Engineering","Systems, Tools","PromptSource","Benchmarks, gold standards, datasets","NLP","Colossal Clean Crawled Corpus (C4)","Issues","Long-tail Knowledge","General, Theory","Emergent Abilities","Language Models","BLOOM","Machine Learning","Open Source","Private Training Data","Training Data Extraction Attack","Training","Scaling","Data-Constrained Scaling Law","Datablations","Data Selection"]}