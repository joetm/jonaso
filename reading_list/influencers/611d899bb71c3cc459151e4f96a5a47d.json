{"docs":[{"title":"Attention Is All You Need","priority":1},{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1},{"title":"SWITCH TRANSFORMERS: SCALING TO TRILLION PARAMETER MODELS WITH SIMPLE AND EFFICIENT SPARSITY","priority":2},{"title":"One Model To Learn Them All","priority":1}],"keywords":["NLP","Transformers","Language Models","Google PaLM","Google Switch-C","Machine Learning","Contrastive Learning, Multimodal Models","Multitask Multimodal Model"]}