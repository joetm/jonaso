{"docs":[{"title":"Attention Is All You Need","priority":1},{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1},{"title":"SWITCH TRANSFORMERS: SCALING TO TRILLION PARAMETER MODELS WITH SIMPLE AND EFFICIENT SPARSITY","priority":2},{"title":"One Model To Learn Them All","priority":1},{"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","priority":2}],"keywords":["NLP","Transformers","Language Models","Google PaLM","Google Switch-C","Machine Learning","Contrastive Learning, Multimodal Models","Multitask Multimodal Model","Benchmarks, gold standards, datasets","Colossal Clean Crawled Corpus (C4)"]}