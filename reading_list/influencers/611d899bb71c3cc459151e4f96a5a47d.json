{"docs":[{"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","priority":2},{"title":"Attention Is All You Need","priority":1},{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1},{"title":"SWITCH TRANSFORMERS: SCALING TO TRILLION PARAMETER MODELS WITH SIMPLE AND EFFICIENT SPARSITY","priority":2},{"title":"GSPMD: General and Scalable Parallelization for ML Computation Graphs","priority":1},{"title":"One Model To Learn Them All","priority":1}],"keywords":["Benchmarks, gold standards, datasets","NLP","Colossal Clean Crawled Corpus (C4)","Transformers","Language Models","PaLM (Google)","Google Switch-C","Machine Learning","Parallelism","Systems, Tools","GSPMD","Contrastive Learning, Multimodal Models","Multitask Multimodal Model"]}