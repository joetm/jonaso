{"docs":[{"title":"Confidence-Building Measures for Artificial Intelligence: Workshop Proceedings","priority":1},{"title":"Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","priority":2},{"title":"OLMo : Accelerating the Science of Language Models","priority":1},{"title":"ZEPHYR: DIRECT DISTILLATION OF LM ALIGNMENT","priority":2},{"title":"A Survey on Data Selection for Language Models","priority":3},{"title":"OLMoE: Open Mixture-of-Experts Language Models","priority":1},{"title":"Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models","priority":1}],"keywords":["AI","Issues","Confidence-Building Measures (CBMs)","Benchmarks, gold standards, datasets","NLP","Dolma","Language Models","OLMo (AI2)","Alignment","Zephyr","Training","Data Selection","Fine-Tuning","Merging, Blending","Mixture of Experts","Open Mixture-of-Experts (OlMoE)","Machine Learning","Generative Deep Learning","Multimodal Generation","Molmo, PixMo"]}