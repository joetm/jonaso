{"docs":[{"title":"ART: Automatic multi-step reasoning and tool-use for large language models","priority":2},{"title":"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping","priority":3},{"title":"SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions","priority":3},{"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?","priority":2},{"title":"Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","priority":2},{"title":"OLMo : Accelerating the Science of Language Models","priority":1},{"title":"SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION","priority":3},{"title":"HUSKY: A Unified, Open-Source Language Agent for Multi-Step Reasoning","priority":2},{"title":"OLMoE: Open Mixture-of-Experts Language Models","priority":1}],"keywords":["Prompt Engineering","Prompt Techniques","Automatic Reasoning and Tool-Use (ART)","NLP","Fine-Tuning","Issues","Random Seeds, Brittleness","Early Stopping","Instruction Tuning","Self-Instruct","In-Context Learning","Meaning","Benchmarks, gold standards, datasets","Dolma","Language Models","OLMo (AI2)","Augmented Language Models","Retrieval-Augmented Generation (RAG)","RAG Optimization, RAG Fine-Tuning, RAG Techniques","Self-RAG","Agents, Autonomous Task Management","Agents","Husky","Merging, Blending","Mixture of Experts","Open Mixture-of-Experts (OlMoE)"]}