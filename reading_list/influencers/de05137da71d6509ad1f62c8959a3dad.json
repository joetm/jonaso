{"docs":[{"title":"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping","priority":3},{"title":"OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models","priority":1},{"title":"The Foundation Model Development Cheatsheet","priority":3},{"title":"Language models scale reliably with over-training and on downstream tasks","priority":2},{"title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus","priority":3}],"keywords":["NLP","Fine-Tuning","Issues","Random Seeds, Brittleness","Early Stopping","Machine Learning","Contrastive Learning, Multimodal Models","OpenFlamingo","Training","General, Theory","Foundation Model Development Cheatsheet","Scaling","Over-Training","Benchmarks, gold standards, datasets","Colossal Clean Crawled Corpus (C4)"]}