{"docs":[{"title":"Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models","priority":2},{"title":"Prompt-Free Diffusion: Taking \u201cText\u201d out of Text-to-Image Diffusion Models","priority":3},{"title":"UNDISTILLABLE: MAKING A NASTY TEACHER THAT CANNOT TEACH STUDENTS","priority":1},{"title":"GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection","priority":2},{"title":"Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk","priority":0}],"keywords":["Machine Learning","Generative Deep Learning","Issues","Unlearning","Forget-Me-Not, Memorization Score (M-Score), ConceptBench","Prompt-Free Diffusion","Knowledge","Knowledge Distillation","Protection","Self-undermining Knowledge Distillation","Nasty Teacher","NLP","Fine-Tuning","Low-Rank Adaptation (LoRA)","GaLore","Diffusion","Privacy","Shake-to-Leak (S2L)"]}