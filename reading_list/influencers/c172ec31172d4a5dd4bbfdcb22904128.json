{"docs":[{"title":"MADLAD-400: A Multilingual And Document-Level Large Audited Dataset","priority":1},{"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","priority":2},{"title":"A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity","priority":2},{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1},{"title":"Extracting Training Data from Large Language Models","priority":1},{"title":"Scalable Extraction of Training Data from (Production) Language Models","priority":3}],"keywords":["Benchmarks, gold standards, datasets","NLP","MADLAD-400","Colossal Clean Crawled Corpus (C4)","Training","Language Models","PaLM (Google)","Machine Learning","Issues","Private Training Data","Training Data Extraction Attack","Security Vulnerabilities","Training Data Extraction","Extractable Memorization"]}