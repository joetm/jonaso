{"docs":[{"title":"PaLM: Scaling Language Modeling with Pathways","priority":1},{"title":"Extracting Training Data from Large Language Models","priority":1},{"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","priority":2},{"title":"A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity","priority":2}],"keywords":["NLP","Language Models","Google PaLM","Machine Learning","Issues","Private Training Data","Training Data Extraction Attack","Benchmarks, gold standards, datasets","Colossal Clean Crawled Corpus (C4)","Training"]}