{"docs":[{"title":"Gotta Catch \u2019Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks","priority":1},{"title":"Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models","priority":3},{"title":"\"RUNNING FAWKES ON YOUR PHOTOS IS LIKE ADDING AN INVISIBLE MASK TO YOUR SELFIES\"","priority":0},{"title":"AI poisoning tool Nightshade received 250,000 downloads in 5 days: \u2018beyond anything we imagined\u2019","priority":1},{"title":"UChicago scientists develop new tool to protect artists from AI mimicry","priority":0},{"title":"Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models","priority":2},{"title":"Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models","priority":2}],"keywords":["NLP","Issues","Adversarial Attacks","Adversarial Example Defense","Honeypots","Trapdoors","Machine Learning","Generative Deep Learning","Data Poisoning","Prompt-specific Poisoning Attacks","Nightshade","Counter Surveillance","Face Recognition","Copyright, Intellectual Property","Protecting Images","Glaze"]}