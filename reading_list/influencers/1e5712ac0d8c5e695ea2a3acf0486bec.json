{"docs":[{"title":"Retentive Network: A Successor to Transformer for Large Language Models","priority":3},{"title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","priority":2}],"keywords":["NLP","Retentive Networks","Optimization","Quantization","1-bit LLMs","BitNet","BitNet b1.58"]}