{"docs":[{"title":"Answering Questions by Meta-Reasoning over Multiple Chains of Thought","priority":2},{"title":"BERTese: Learning to Speak to BERT","priority":2},{"title":"Transformer Feed-Forward Layers Are Key-Value Memories","priority":1},{"title":"Crawling The Internal Knowledge-Base of Language Models","priority":3}],"keywords":["Prompt Engineering","Prompt Techniques","Chain of Thought, CoT","Multi-Chain Reasoning (MCR)","Paraphrasing","BERTese","NLP","Transformers","Augmented Language Models","Retrieval-Augmented Generation (RAG)","Parametric Knowledge","Knowledge Graph Crawling"]}