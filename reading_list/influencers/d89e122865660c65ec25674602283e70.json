{"docs":[{"title":"Cut the CARP: Fishing for zero-shot story evaluation","priority":1},{"title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling","priority":2},{"title":"Transformer Math 101","priority":1},{"title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling","priority":1},{"title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","priority":1},{"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","priority":1},{"title":"EleutherAI's Thoughts on the EU AI Act","priority":0},{"title":"Neural Language Models are Effective Plagiarists","priority":3},{"title":"VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance","priority":3},{"title":"Why are some LLMs trained on both CommonCrawl and Wikipedia\/StackExchange?","priority":0},{"title":"LEACE: Perfect linear concept erasure in closed form","priority":2},{"title":"The Foundation Model Development Cheatsheet","priority":3}],"keywords":["Hypertext, Hypermedia","Narrative, Story, Fabula","Story Evaluation","Contrastive Authoring and Reviewing Pairing (CARP)","Benchmarks, gold standards, datasets","NLP","The Pile","Transformers","Pythia","Language Models","GPT","GPT-Neo","GPT-NeoX-20B","BLOOM","Machine Learning","Issues","Open Source","Amended EU AI Act","Plagiarism","Generative Deep Learning","Text-to-Image","VQGAN+CLIP","Training","Training Data","Concept Erasure","Concept Scrubbing","LEAst-squares Concept Erasure (LEACE)","General, Theory","Foundation Model Development Cheatsheet"]}