{"docs":[{"title":"On the Opportunities and Risks of Foundation Models","priority":3},{"title":"Extracting Training Data from Large Language Models","priority":1},{"title":"Extracting Training Data from Diffusion Models","priority":3},{"title":"Scalable Extraction of Training Data from (Production) Language Models","priority":3},{"title":"Stealing Part of a Production Language Model","priority":1},{"title":"Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI","priority":3},{"title":"AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents","priority":2}],"keywords":["AI","Foundation Models","Machine Learning","Issues","Private Training Data","Training Data Extraction Attack","Generative Deep Learning","Diffusion","Memorization of Training Data","NLP","Security Vulnerabilities","Training Data Extraction","Extractable Memorization","Security","Model-Stealing Attacks","Last Layer","Copyright, Intellectual Property","Protecting Images","Adversarial Pertubations","Prompt Engineering","Agents, Autonomous Task Management","Prompt Injection","AgentDojo"]}