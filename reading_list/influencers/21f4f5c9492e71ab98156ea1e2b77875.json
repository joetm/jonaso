{"docs":[{"title":"SPECTER: Document-level Representation Learning using Citation-informed Transformers","priority":1},{"title":"From Who You Know to What You Read: Augmenting Scientific Recommendations with Implicit Social Networks","priority":3},{"title":"Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks","priority":1}],"keywords":["NLP","Language Models","Specter","Future Science","Recommendations","Machine Learning","General, Theory","Training"]}