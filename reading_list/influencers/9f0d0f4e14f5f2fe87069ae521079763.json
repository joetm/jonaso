{"docs":[{"title":"ATTENTION SINKS AND COMPRESSION VALLEYS IN LLMS ARE TWO SIDES OF THE SAME COIN","priority":2},{"title":"LLMs can hide text in other text of the same length","priority":2}],"keywords":["NLP","Transformers","Attention Sinks, Compression Valleys","Mix-Compress-Refine","Applications","Encryption, Cyphering"]}