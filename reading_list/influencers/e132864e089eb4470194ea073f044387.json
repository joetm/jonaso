{"docs":[{"title":"LIMA: Less Is More for Alignment","priority":2},{"title":"Self-Alignment with Instruction Backtranslation","priority":3},{"title":"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor","priority":1},{"title":"Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation","priority":3},{"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","priority":1},{"title":"Transformer Feed-Forward Layers Are Key-Value Memories","priority":1}],"keywords":["AI","Issues","Human-AI Alignment","LIMA","Self-Alignment","Instruction Backtranslation","Prompt Engineering","Prompt Learning","Soft Prompts","Instruction Tuning","Unnatural Instructions","Benchmarks, gold standards, datasets","Images","Aesthetics","Pick-a-Pic","PickScore","NLP","Language Models","BART","Transformers"]}