{"docs":[{"title":"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor","priority":1},{"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","priority":1},{"title":"LIMA: Less Is More for Alignment","priority":2}],"keywords":["Prompt Engineering","Prompt Learning","Soft Prompts","Instruction Tuning","Unnatural Instructions","NLP","Language Models","BART","AI","Issues","Human-AI Alignment","LIMA"]}